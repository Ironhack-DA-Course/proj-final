{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mticker\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgraphviz\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvis\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import graphviz\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "%matplotlib inline\n",
    "\n",
    "# Stats\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as st\n",
    "from scipy.stats import shapiro, norm, chi2_contingency, kstest, boxcox\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder,PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import  accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, classification_report #Classifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error # Regressor\n",
    "\n",
    "#lib\n",
    "\n",
    "#\n",
    "# from wordcloud import WordCloud,STOPWORDS\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "\n",
    "# os\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "# time\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")    # (Optional)\n",
    "\n",
    "print(\"Project has been created with Pandas: \" ,pd. __version__,\" And with Numpy: \",np. __version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "try:\n",
    "    with open(\"../config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except:\n",
    "    print(\"Yaml configuration file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(config[\"data\"][\"clean\"][\"file_eda_cleaned\"])\n",
    "# df = df.sort_values(by = [\"ext_install_count\", \"ext_rating\"], ascending= False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature transformation/Transform values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle categories with high cardinality -> Grouping rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[\"ext_categories\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[\"repo_languages\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "threshold_ext_categories = 0.1  # e.g., categories below 10% frequency\n",
    "value_counts_ext_categories = df[\"ext_categories\"].value_counts(normalize=True)\n",
    "rare_categories = value_counts_ext_categories[value_counts_ext_categories.values <= threshold_ext_categories].index\n",
    "\n",
    "def transform_ext_categories(x):\n",
    "    text = str(x)\n",
    "    if text == \"Other\":\n",
    "        return \"Unknown\"\n",
    "    elif text in rare_categories:\n",
    "        return \"Others\"\n",
    "    else:\n",
    "        return x\n",
    "df[\"ext_categories_grouped\"] = df[\"ext_categories\"].apply(transform_ext_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "threshold_repo_languages = 0.05 # e.g., categories below 3% frequency\n",
    "value_counts_repo_languages = df[\"repo_languages\"].value_counts(normalize=True)\n",
    "rare_categories = value_counts_repo_languages[value_counts_repo_languages.values <= threshold_repo_languages].index\n",
    "\n",
    "def transform_repo_languages(x):\n",
    "    text = str(x)\n",
    "    if text  in [\"other\", \"unknown\"]:\n",
    "        return \"unknown\"\n",
    "    elif text in rare_categories:\n",
    "        return \"others\"\n",
    "    else:\n",
    "        return x\n",
    "df[\"repo_languages_grouped\"] = df[\"repo_languages\"].apply(transform_repo_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# threshold_ext_categories = 0.1  # e.g., categories below 10% frequency\n",
    "# value_counts_ext_categories = df['ext_categories'].value_counts(normalize=True)\n",
    "# rare_categories = value_counts_ext_categories[value_counts_ext_categories.values <= threshold_ext_categories].index\n",
    "# df['ext_categories_grouped'] = df['ext_categories'].apply(lambda x: 'Rest' if x in rare_categories else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# threshold_repo_languages = 0.03  # e.g., categories below 3% frequency\n",
    "# value_counts_repo_languages = df[\"repo_languages\"].value_counts(normalize=True)\n",
    "# rare_categories = value_counts_repo_languages[value_counts_repo_languages.values <= threshold_repo_languages].index\n",
    "# df[\"repo_languages_grouped\"] = df[\"repo_languages\"].apply(lambda x: 'rest' if x in rare_categories else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[\"ext_categories_grouped\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[\"repo_languages_grouped\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"repo_languages\", \"ext_categories\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert target to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[\"verified\"] = df[\"verified\"].map({True: 1, False:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get number and category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "potential_categorical_from_numerical = df.select_dtypes(\"number\").loc[:, df.select_dtypes(\"number\").nunique() < 10].drop(columns=\"verified\")\n",
    "potential_categorical_from_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "df_categorical = pd.concat([df.select_dtypes(\"object\"), potential_categorical_from_numerical], axis=1)\n",
    "df_numerical = df.select_dtypes(\"number\").drop(columns=potential_categorical_from_numerical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cols_num = df_numerical.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cols_num = df_numerical.drop([\"verified\",\"total_vulners\"],axis=1).columns.to_list() #Drop 'total_vulners'\n",
    "cols_cat = df_categorical.columns.to_list()\n",
    "cols_num, cols_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "features = df.drop(columns = [\"verified\"])\n",
    "target = df[\"verified\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.20, random_state=0) #before transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OHE: for nominal categorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe.fit((X_train[[\"repo_languages_grouped\", \"ext_categories_grouped\"]]))\n",
    "X_train_trans_nom_np = ohe.transform(X_train[[\"repo_languages_grouped\", \"ext_categories_grouped\"]])\n",
    "X_test_trans_nom_np = ohe.transform(X_test[[\"repo_languages_grouped\", \"ext_categories_grouped\"]])\n",
    "\n",
    "X_train_nom_trans_df = pd.DataFrame(X_train_trans_nom_np, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "X_test_nom_trans_df = pd.DataFrame(X_test_trans_nom_np, columns=ohe.get_feature_names_out(), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train_nom_trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(config[\"model\"][\"preprocessing\"][\"file_ohe\"], \"wb\") as file:\n",
    "    pickle.dump(ohe, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform cols_num to normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows = int(np.ceil(len(cols_num)/2)), ncols = 2, figsize = (8,10))\n",
    "axes = axes.flat\n",
    "\n",
    "for i,col in enumerate(cols_num):\n",
    "    sm.qqplot(X_test[col], \n",
    "           line = \"s\", \n",
    "           ax = axes[i])\n",
    "    \n",
    "    axes[i].set_title(col, fontsize = 10, fontweight = \"bold\", color = \"black\")\n",
    "    \n",
    "# fig.delaxes(axes[7])\n",
    "fig.suptitle(\"QQ-Plots before Transforming\", fontsize = 12, fontweight = \"bold\", color = \"darkblue\")\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powertransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Power transform\n",
    "pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "X_train_num = X_train[cols_num]\n",
    "pt.fit(X_train_num)\n",
    "X_test_num  = X_test[cols_num]\n",
    "\n",
    "X_train_num_trans = pt.transform(X_train_num)\n",
    "X_test_num_trans = pt.transform(X_test_num)\n",
    "\n",
    "X_train_num_trans_df = pd.DataFrame(X_train_num_trans, columns=X_train_num.columns, index=X_train_num.index )\n",
    "X_test_num_trans_df = pd.DataFrame(X_test_num_trans, columns=X_test_num.columns, index=X_test_num.index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# #Normalizer\n",
    "# normalizer = MinMaxScaler()\n",
    "# X_train_num = X_train[cols_num]\n",
    "# normalizer.fit(X_train_num)\n",
    "# X_test_num  = X_test[cols_num]\n",
    "\n",
    "# X_train_trans = normalizer.transform(X_train_num)\n",
    "# X_test_trans = normalizer.transform(X_test_num)\n",
    "\n",
    "# X_train_trans = pd.DataFrame(X_train_trans, columns=X_train_num.columns, index=X_train_num.index)\n",
    "# X_test_trans = pd.DataFrame(X_test_trans, columns=X_test_num.columns, index=X_test_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# #Log-transform\n",
    "# for col in cols_num:\n",
    "#     df[col] = np.log1p(df[col])\n",
    "    # X_train_trans = pt.transform(X_train_num)\n",
    "    # X_test_trans = pt.transform(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows = int(np.ceil(len(cols_num)/2)), ncols = 2, figsize = (8,10))\n",
    "axes = axes.flat\n",
    "\n",
    "for i,col in enumerate(cols_num):\n",
    "    sns.histplot(X_test_num_trans_df[col],\n",
    "                 kde=True,\n",
    "                 bins=20,\n",
    "                 color=\"orange\",\n",
    "                 ax=axes[i]) \n",
    "    \n",
    "    # sm.qqplot(X_test_num_trans_df[col], \n",
    "    #        line = \"s\", \n",
    "    #        ax = axes[i]);\n",
    "    \n",
    "    axes[i].set_title(col, fontsize = 10, fontweight = \"bold\", color = \"black\")\n",
    "    \n",
    "# fig.delaxes(axes[7])\n",
    "fig.suptitle(\"QQ-Plots after Transforming\", fontsize = 12, fontweight = \"bold\", color = \"darkblue\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train_ord_trans_df = X_train[[\"ext_rating_category\",\"ext_version_category\"]].copy()\n",
    "X_test_ord_trans_df = X_test[[\"ext_rating_category\",\"ext_version_category\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train_trans = pd.concat([X_train_num_trans_df, X_train_nom_trans_df, X_train_ord_trans_df], axis=1)\n",
    "X_test_trans = pd.concat([X_test_num_trans_df, X_test_nom_trans_df, X_test_ord_trans_df], axis=1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_trans =  pd.concat([X_train_trans, X_test_trans], axis = 0)\n",
    "y_trans = pd.concat([y_train, y_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# df_trans = pd.concat([X_trans,y_trans], axis = 1).reset_index(drop= True)\n",
    "# df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# X_train_trans.shape[0] == X_train.shape[0]\n",
    "# X_test_trans.shape[0] == X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# X_train_corr = pd.concat([X_train_trans, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "corr=np.abs(X_train_trans.corr(method=\"pearson\"))\n",
    "\n",
    "# Set up mask for triangle representation\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "# sns.heatmap(corr, vmax=1,square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot = corr)\n",
    "sns.heatmap(corr, mask=mask,  vmax=1,square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot = corr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are high correlations between **_repo stars vs repo forks, total_vulners vs low vulvers /medium vulners_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# df_trans= df_trans.drop(\"total_vulners\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# features_ = df_trans.drop(columns = [\"verified\"])\n",
    "# target_ = df_trans[\"verified\"]\n",
    "# Xtrans_train, Xtrans_test, ytrans_train, ytrans_test = train_test_split(features,target, test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state = 1,sampling_strategy=1.0)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# def bar_labels(axes, rotation=0, location=\"edge\", xlabel=None, ylabel=None):\n",
    "#     \"\"\"Add labels to bars in a bar plot and configure axes.\"\"\"\n",
    "#     for container in axes.containers:\n",
    "#         axes.bar_label(container, label_type=location, rotation=rotation)\n",
    "#     if xlabel:\n",
    "#         axes.set_xlabel(xlabel)\n",
    "#     if ylabel:\n",
    "#         axes.set_ylabel(ylabel)\n",
    "#     else:\n",
    "#         axes.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "# def training_classification_optimized(x_train: pd.DataFrame | np.ndarray , x_test: pd.DataFrame | np.ndarray, y_train: pd.DataFrame | np.ndarray, y_test: pd.DataFrame | np.ndarray):\n",
    "#     \"\"\"Train and evaluate multiple classifiers, plot accuracies and confusion matrices.\"\"\"\n",
    "    \n",
    "#     # Define models dictionary\n",
    "#     models = {\n",
    "#         \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "#         \"Ada Boost\": AdaBoostClassifier(random_state=0),\n",
    "#         \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "#         \"Extra Trees\": ExtraTreesClassifier(random_state=0),\n",
    "#         \"Logistic Regression\": LogisticRegression(random_state=0, max_iter=1000),\n",
    "#         \"SVC\": SVC(random_state=0),\n",
    "#         \"XGBoost\": XGBClassifier(random_state=0),\n",
    "#         \"LightGBM\": LGBMClassifier(verbose=-100, random_state=0),\n",
    "#         \"Cat Boost\": CatBoostClassifier(verbose=False, random_state=0)\n",
    "#     }\n",
    "\n",
    "#     # Initialize storage for metrics\n",
    "#     scores = {}\n",
    "#     cms = {}\n",
    "#     reports = {}\n",
    "\n",
    "#     # Train and evaluate each model\n",
    "#     for name, model in models.items():\n",
    "#         model.fit(x_train, y_train)\n",
    "#         with open(name + \".pkl\", \"wb\") as file:\n",
    "#             pickle.dump(model, file)\n",
    "#         pred = model.predict(x_test)\n",
    "#         scores[name] = accuracy_score(y_test, pred) * 100  # Convert to percentage\n",
    "#         cms[name] = confusion_matrix(y_test, pred)\n",
    "#         # f1_score(y_test, y_pred_rf, average='binary')\n",
    "#         # roc_auc_score(y_test, y_pred_proba_rf[:,1])]\n",
    "#         reports[name] = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "#     # Create DataFrame for scores\n",
    "#     dt = pd.DataFrame.from_dict(scores, orient='index', columns=['scores'])\n",
    "#     dt = dt.sort_values('scores', ascending=False)\n",
    "#     dt['scores'] = dt['scores'].round(2)\n",
    "\n",
    "#     # Plot accuracy bar chart\n",
    "#     fig, ax = plt.subplots(figsize=(15, 6))\n",
    "#     dt['scores'].plot(kind='bar', ax=ax)\n",
    "#     bar_labels(ax, xlabel='Model', ylabel='Accuracy (%)')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f\"{'*'*30}\\n\")\n",
    "    \n",
    "#     # Plot confusion matrices in rows of up to 5\n",
    "#     index = 0\n",
    "#     n_models = len(models)\n",
    "#     while index < n_models:\n",
    "#         n_cols = min(5, n_models - index)\n",
    "#         fig, axes = plt.subplots(ncols=n_cols, figsize=(3 * n_cols, 4))\n",
    "#         if n_cols == 1:\n",
    "#             axes = [axes]  # Ensure axes is iterable for single plot\n",
    "#         for i in range(n_cols):\n",
    "#             model_name = dt.index[index]\n",
    "#             sns.heatmap(cms[model_name], annot=True, fmt='d', ax=axes[i], cbar=False)\n",
    "#             axes[i].set_title(f\"{model_name}: {dt.loc[model_name, 'scores']}%\")\n",
    "#             axes[i].set_xlabel('Predicted')\n",
    "#             axes[i].set_ylabel('True')\n",
    "#             index += 1\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#     # Print classification reports\n",
    "#     for name in dt.index:\n",
    "#         print(f\"{'*'*30}\\n{name}\\n\")\n",
    "#         print(pd.DataFrame(reports[name]).transpose().round(2))\n",
    "\n",
    "# training_classification_optimized(X_train_trans, X_test_trans, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union, Optional\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_importance(\n",
    "    model: Any,\n",
    "    x_train: Union[pd.DataFrame, np.ndarray],\n",
    "    y_train: Union[pd.Series, np.ndarray],\n",
    "    x_test: Optional[Union[pd.DataFrame, np.ndarray]] = None,\n",
    "    y_test: Optional[Union[pd.Series, np.ndarray]] = None,\n",
    "    method: str = 'auto',\n",
    "    feature_names: Optional[list] = None,\n",
    "    top_n: int = 20,\n",
    "    plot: bool = False,  # Changed default to False for batch processing\n",
    "    figsize: tuple = (15, 10),\n",
    "    random_state: int = 0\n",
    ") -> pd.DataFrame:\n",
    "   \n",
    "    # Get feature names\n",
    "    if feature_names is None:\n",
    "        if hasattr(x_train, 'columns'):\n",
    "            feature_names = x_train.columns.tolist()\n",
    "        else:\n",
    "            feature_names = [f'feature_{i}' for i in range(x_train.shape[1])]\n",
    "\n",
    "    # Determine method automatically if 'auto'\n",
    "    if method == 'auto':\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            method = 'builtin'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            method = 'coefficients'\n",
    "        else:\n",
    "            method = 'permutation'\n",
    "            if x_test is None or y_test is None:\n",
    "                warnings.warn(\"No test data provided. Using training data for permutation importance (may overfit).\")\n",
    "                x_test, y_test = x_train, y_train\n",
    "\n",
    "    importance_scores = None\n",
    "    importance_std = None\n",
    "    method_name = \"\"\n",
    "\n",
    "    # Calculate importance based on method\n",
    "    if method == 'builtin':\n",
    "        # Tree-based models (RandomForest, XGBoost, LightGBM, etc.)\n",
    "        importance_scores = model.feature_importances_\n",
    "        method_name = \"Built-in Feature Importance\"\n",
    "\n",
    "    elif method == 'coefficients':\n",
    "        # Linear models (LogisticRegression, LinearRegression, etc.)\n",
    "        if hasattr(model, 'coef_'):\n",
    "            coef = model.coef_\n",
    "            if coef.ndim > 1:\n",
    "                # Multi-class classification - take mean of absolute values\n",
    "                importance_scores = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                importance_scores = np.abs(coef)\n",
    "            method_name = \"Coefficient-based Importance\"\n",
    "        else:\n",
    "            raise ValueError(\"Model does not have coefficients. Try 'permutation' method.\")\n",
    "\n",
    "    elif method == 'permutation':\n",
    "        # Permutation importance - works with any model\n",
    "        try:\n",
    "            # Use test data if available, otherwise training data\n",
    "            x_eval = x_test if x_test is not None else x_train\n",
    "            y_eval = y_test if y_test is not None else y_train\n",
    "\n",
    "            perm_importance = permutation_importance(\n",
    "                model, x_eval, y_eval,\n",
    "                n_repeats=5,  # Reduced for faster processing\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            importance_scores = perm_importance.importances_mean\n",
    "            importance_std = perm_importance.importances_std\n",
    "            method_name = \"Permutation Importance\"\n",
    "\n",
    "        except ImportError:\n",
    "            raise ImportError(\"sklearn is required for permutation importance\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'auto', 'builtin', 'permutation', or 'coefficients'\")\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_data = {\n",
    "        'feature': feature_names,\n",
    "        'importance': importance_scores\n",
    "    }\n",
    "\n",
    "    if importance_std is not None:\n",
    "        results_data['importance_std'] = importance_std\n",
    "\n",
    "    importance_df = pd.DataFrame(results_data).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Create visualization\n",
    "    if plot:\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "        # Get top N features\n",
    "        top_features = importance_df.head(top_n)\n",
    "\n",
    "        # Create horizontal bar plot\n",
    "        y_pos = np.arange(len(top_features))\n",
    "        bars = plt.barh(y_pos, top_features['importance'], alpha=0.8)\n",
    "\n",
    "        # Add error bars if available\n",
    "        if 'importance_std' in top_features.columns:\n",
    "            plt.errorbar(top_features['importance'], y_pos,\n",
    "                        xerr=top_features['importance_std'],\n",
    "                        fmt='none', color='black', alpha=0.6)\n",
    "\n",
    "        # Customize plot\n",
    "        plt.yticks(y_pos, top_features['feature'])\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.title(f'{method_name} - Top {min(top_n, len(top_features))} Features')\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):\n",
    "            plt.text(bar.get_width() + max(top_features['importance']) * 0.01,\n",
    "                    bar.get_y() + bar.get_height()/2,\n",
    "                    f'{importance:.4f}',\n",
    "                    va='center', fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# results1 = training_classification_optimized(X_train_trans, X_test_trans, y_train, y_test)\n",
    "# results2 = training_classification_optimized(X_train_smote, X_test_trans, y_train_smote, y_test)\n",
    "# comparison = compare_experiments([results1, results2], ['Original', 'SMOTE'], 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_labels(axes, rotation=0, location=\"edge\", xlabel=None, ylabel=None):\n",
    "    \"\"\"Add labels to bars in a bar plot and configure axes.\"\"\"\n",
    "    for container in axes.containers:\n",
    "        axes.bar_label(container, label_type=location, rotation=rotation)\n",
    "    if xlabel:\n",
    "        axes.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        axes.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def plot_metrics_comparison(metrics_dict, metric_names, figsize=(20, 12)):\n",
    "    \"\"\"Plot multiple metrics in subplots.\"\"\"\n",
    "    n_metrics = len(metric_names)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            df_metric = pd.DataFrame.from_dict(metrics_dict[metric_name], orient='index', \n",
    "                                             columns=[metric_name])\n",
    "            df_metric = df_metric.sort_values(metric_name, ascending=False)\n",
    "            df_metric[metric_name] = df_metric[metric_name].round(2)\n",
    "            \n",
    "            df_metric[metric_name].plot(kind='bar', ax=ax, color='skyblue')\n",
    "            bar_labels(ax, xlabel='Model', ylabel=f'{metric_name.replace(\"_\", \" \").title()} (%)')\n",
    "            ax.set_title(f'{metric_name.replace(\"_\", \" \").title()} Comparison')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(metric_names), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(cms, df_metrics, metric_for_title='Accuracy'):\n",
    "    \"\"\"Plot confusion matrices in rows of up to 5.\"\"\"\n",
    "    model_names = list(cms.keys())\n",
    "    n_models = len(model_names)\n",
    "    index = 0\n",
    "    \n",
    "    while index < n_models:\n",
    "        n_cols = min(5, n_models - index)\n",
    "        fig, axes = plt.subplots(ncols=n_cols, figsize=(3 * n_cols, 4))\n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i in range(n_cols):\n",
    "            if index < n_models:\n",
    "                model_name = model_names[index]\n",
    "                sns.heatmap(cms[model_name], annot=True, fmt='d', ax=axes[i], cbar=False)\n",
    "                \n",
    "                # Get metric value for title\n",
    "                metric_value = df_metrics.loc[model_name, metric_for_title] if model_name in df_metrics.index else 'N/A'\n",
    "                axes[i].set_title(f\"{model_name}: {metric_value}%\")\n",
    "                axes[i].set_xlabel('Predicted')\n",
    "                axes[i].set_ylabel('True')\n",
    "                index += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_comparison(importances_dict, top_n=10, figsize=(20, 15)):\n",
    "    \"\"\"Plot feature importance comparison across models.\"\"\"\n",
    "    n_models = len(importances_dict)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for i, (model_name, importance_df) in enumerate(importances_dict.items()):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            top_features = importance_df.head(top_n)\n",
    "            \n",
    "            y_pos = np.arange(len(top_features))\n",
    "            bars = ax.barh(y_pos, top_features['importance'], alpha=0.8)\n",
    "            \n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(top_features['feature'], fontsize=8)\n",
    "            ax.set_xlabel('Importance Score')\n",
    "            ax.set_title(f'{model_name} - Top {top_n} Features')\n",
    "            ax.invert_yaxis()\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, (bar, importance) in enumerate(zip(bars, top_features['importance'])):\n",
    "                ax.text(bar.get_width() + max(top_features['importance']) * 0.01,\n",
    "                       bar.get_y() + bar.get_height()/2,\n",
    "                       f'{importance:.3f}',\n",
    "                       va='center', fontsize=6)\n",
    "            \n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(importances_dict), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_classification_optimized(x_train, x_test, y_train, y_test, \n",
    "                                    save_models=True, plot_feature_importance=True,\n",
    "                                    feature_importance_top_n=10):\n",
    "    \"\"\"Train and evaluate multiple classifiers with comprehensive metrics and feature importance.\"\"\"\n",
    "    \n",
    "    # Define models dictionary\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "        \"Ada Boost\": AdaBoostClassifier(random_state=0),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=0),\n",
    "        \"Logistic Regression\": LogisticRegression(random_state=0, max_iter=1000),\n",
    "        \"SVC\": SVC(random_state=0, probability=True),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(random_state=0, eval_metric='logloss'),\n",
    "        \"LightGBM\": LGBMClassifier(verbose=-1, random_state=0),\n",
    "        \"CatBoost\": CatBoostClassifier(verbose=False, random_state=0)\n",
    "    }\n",
    "    \n",
    "    # Define metrics to calculate\n",
    "    metrics = {\n",
    "        'Accuracy': {},\n",
    "        'Precision': {},\n",
    "        'Recall': {},\n",
    "        'F1_Score': {},\n",
    "        'ROC_AUC': {}\n",
    "    }\n",
    "    \n",
    "    # Additional storage\n",
    "    cms = {}\n",
    "    reports = {}\n",
    "    importances = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    print(\"Training models and calculating feature importance...\")\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Fit model\n",
    "        model.fit(x_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Save model if requested\n",
    "        if save_models:\n",
    "            with open(f\"{name.replace(' ', '_')}.pkl\", \"wb\") as file:\n",
    "                pickle.dump(model, file)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        # Get probabilities for ROC-AUC (handle models without predict_proba)\n",
    "        try:\n",
    "            y_proba = model.predict_proba(x_test)[:, 1]\n",
    "        except AttributeError:\n",
    "            y_proba = y_pred\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics['Accuracy'][name] = accuracy_score(y_test, y_pred) * 100\n",
    "        metrics['Precision'][name] = precision_score(y_test, y_pred, average='binary') * 100\n",
    "        metrics['Recall'][name] = recall_score(y_test, y_pred, average='binary') * 100\n",
    "        metrics['F1_Score'][name] = f1_score(y_test, y_pred, average='binary') * 100\n",
    "        \n",
    "        try:\n",
    "            metrics['ROC_AUC'][name] = roc_auc_score(y_test, y_proba) * 100\n",
    "        except (ValueError, TypeError):\n",
    "            metrics['ROC_AUC'][name] = 0\n",
    "        \n",
    "        # Store confusion matrix and classification report\n",
    "        cms[name] = confusion_matrix(y_test, y_pred)\n",
    "        reports[name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Get feature importance using the optimized function\n",
    "        try:\n",
    "            if name in [\"Logistic Regression\"]:\n",
    "                importances[name] = get_feature_importance(\n",
    "                    model, x_train, y_train, x_test, y_test, \n",
    "                    method='coefficients', top_n=feature_importance_top_n\n",
    "                )\n",
    "            elif name in [\"SVC\", \"KNN\"]:\n",
    "                importances[name] = get_feature_importance(\n",
    "                    model, x_train, y_train, x_test, y_test,\n",
    "                    method='permutation', top_n=feature_importance_top_n\n",
    "                )\n",
    "            else:\n",
    "                importances[name] = get_feature_importance(\n",
    "                    model, x_train, y_train, x_test, y_test,\n",
    "                    method='builtin', top_n=feature_importance_top_n\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate feature importance for {name}: {e}\")\n",
    "            # Create dummy importance for consistency\n",
    "            n_features = x_train.shape[1]\n",
    "            feature_names = (x_train.columns.tolist() if hasattr(x_train, 'columns') \n",
    "                           else [f'feature_{i}' for i in range(n_features)])\n",
    "            importances[name] = pd.DataFrame({\n",
    "                'feature': feature_names[:feature_importance_top_n],\n",
    "                'importance': np.zeros(min(feature_importance_top_n, n_features))\n",
    "            })\n",
    "    \n",
    "    # Create comprehensive results DataFrame\n",
    "    results_df = pd.DataFrame(metrics).round(2)\n",
    "    results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OVERALL RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(results_df)\n",
    "    \n",
    "    # Plot individual metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PLOTTING INDIVIDUAL METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metric_names = list(metrics.keys())\n",
    "    plot_metrics_comparison(metrics, metric_names)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFUSION MATRICES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    plot_confusion_matrices(cms, results_df, metric_for_title='Accuracy')\n",
    "    \n",
    "    # Plot feature importance comparison\n",
    "    if plot_feature_importance and importances:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE IMPORTANCE COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        plot_feature_importance_comparison(importances, top_n=feature_importance_top_n)\n",
    "    \n",
    "    # Print detailed classification reports\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name in results_df.index:\n",
    "        print(f\"\\n{'*'*40}\\n{name}\\n{'*'*40}\")\n",
    "        report_df = pd.DataFrame(reports[name]).transpose().round(2)\n",
    "        print(report_df)\n",
    "    \n",
    "    # Print top feature importance for each model\n",
    "    if importances:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TOP FEATURE IMPORTANCE BY MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, importance_df in importances.items():\n",
    "            print(f\"\\n{'-'*30}\\n{name} - Top {min(5, len(importance_df))} Features\\n{'-'*30}\")\n",
    "            print(importance_df.head().to_string(index=False))\n",
    "    \n",
    "    # Return results for further analysis\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'results_df': results_df,\n",
    "        'confusion_matrices': cms,\n",
    "        'classification_reports': reports,\n",
    "        'feature_importances': importances,\n",
    "        'trained_models': trained_models\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for comparing multiple experiments\n",
    "def compare_experiments(results_list, experiment_names, metric='Accuracy'):\n",
    "    \"\"\"Compare results from multiple experiments.\"\"\"\n",
    "    comparison_df = pd.DataFrame()\n",
    "    \n",
    "    for i, (results, exp_name) in enumerate(zip(results_list, experiment_names)):\n",
    "        comparison_df[exp_name] = results['results_df'][metric]\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    comparison_df.plot(kind='bar', ax=ax)\n",
    "    bar_labels(ax, xlabel='Model', ylabel=f'{metric} (%)')\n",
    "    ax.set_title(f'{metric} Comparison Across Experiments')\n",
    "    ax.legend(title='Experiments')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# def plot_histograms_by_diagnosis(df, target='diagnosis', exclude_cols=None, bins=30, cols_per_row=3):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import seaborn as sns\n",
    "\n",
    "#     if exclude_cols is None:\n",
    "#         exclude_cols = []\n",
    "\n",
    "#     # Select only numerical columns except the target and excluded ones\n",
    "#     num_cols = df.select_dtypes(include='number').columns.difference(exclude_cols + [target])\n",
    "#     n = len(num_cols)\n",
    "#     nrows = (n + cols_per_row - 1) // cols_per_row\n",
    "\n",
    "#     fig, axes = plt.subplots(nrows, cols_per_row, figsize=(6 * cols_per_row, 4 * nrows))\n",
    "#     axes = axes.flatten()\n",
    "\n",
    "#     for i, col in enumerate(num_cols):\n",
    "#         for diagnosis in df[target].unique():\n",
    "#             sns.histplot(df[df[target] == diagnosis][col],\n",
    "#                          label=str(diagnosis),\n",
    "#                          bins=bins,\n",
    "#                          kde=False,\n",
    "#                          ax=axes[i],\n",
    "#                          element='step',\n",
    "#                          stat='density')\n",
    "\n",
    "#         axes[i].set_title(f'{col} distribution by {target}', fontsize=10)\n",
    "#         axes[i].legend(title=target)\n",
    "\n",
    "#     for j in range(i + 1, len(axes)):\n",
    "#         fig.delaxes(axes[j])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# models = {\n",
    "#     \"DT\":  DecisionTreeClassifier(criterion= 'entropy', max_depth= 10, max_features= None, min_samples_leaf= 4, min_samples_split= 2, splitter= 'best'),\n",
    "#     \"KNN\": KNeighborsClassifier(n_neighbors=3,p= 1, weights= 'distance'),\n",
    "#     \"RF\":  RandomForestClassifier( max_depth=20,max_features=7),\n",
    "#     \"SGD\": SGDClassifier(early_stopping=True ,loss='log' ,eta0=0.001, random_state=42),\n",
    "#     \"XGB\": XGBClassifier(learning_rate= 0.1, max_depth= 7, n_estimators=200)\n",
    "# }\n",
    "\n",
    "# MODELS_RESULTS = pd.DataFrame(\n",
    "#     columns=['Model', 'Sampling By ','Train Score', 'Test Score', 'Recall', 'Precision', 'f1-score', 'classification_report'])\n",
    "\n",
    "\n",
    "# def test_models(X_train, X_test, y_train, y_test, Sampling_tech):\n",
    "\n",
    "#     for model in models:\n",
    "#         # fit\n",
    "#         fit = models[model].fit(X_train, y_train)\n",
    "#         # sep before output\n",
    "#         print('-'*40)\n",
    "#         print(f' -------------------{model}-------------------')\n",
    "#         print('-'*40)\n",
    "\n",
    "#         # output\n",
    "#         y_pred= fit.predict(X_test)\n",
    "#         report = metrics.classification_report(y_test,y_pred)\n",
    "\n",
    "#         print(report)\n",
    "#         fig, ax = plt.subplots(figsize=(6, 4))\n",
    "#         ax.set_title(model)\n",
    "#         sns.heatmap(metrics.confusion_matrix(y_test, y_pred),\n",
    "#                     annot=True, cmap='Blues', fmt='g', cbar=False, ax=ax)\n",
    "#         plt.xlabel('Predicted labels')\n",
    "#         plt.ylabel('True labels')\n",
    "#         plt.show()\n",
    "\n",
    "#         # data frame of MODELS_RESULTS and save results\n",
    "#         train_score = round(fit.score(X_train, y_train), 3)\n",
    "#         test_score = round(fit.score(X_test, y_test), 3)\n",
    "#         recall = round(metrics.recall_score(y_test, y_pred) * 100, 2)\n",
    "#         precision = round(metrics.precision_score(y_test,y_pred) * 100, 2)\n",
    "#         f1 = round(metrics.f1_score(y_test, y_pred) * 100, 2)\n",
    "\n",
    "#         MODELS_RESULTS.loc[len(MODELS_RESULTS.index)] = [model, Sampling_tech,\n",
    "#                                            train_score, test_score, recall, precision, f1, report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Random Forest\n",
    "\n",
    "Ensemble of decision trees (bagging).\n",
    "Uses random subsets of data and features.\n",
    "Robust to overfitting and outliers.\n",
    "Good baseline model for tabular data.\n",
    "-  AdaBoost\n",
    "\n",
    "Sequential boosting of weak learners.\n",
    "Focuses on previous misclassified samples.\n",
    "Sensitive to noise/outliers.\n",
    "Good for clean data with subtle patterns.\n",
    "-  XGBoost\n",
    "\n",
    "Optimized gradient boosting algorithm.\n",
    "Fast, accurate, and regularized.\n",
    "Best for performance with tuning effort.\n",
    "\n",
    "-  Logistic Regression\n",
    "\n",
    "Linear model for binary classification.\n",
    "Estimates probabilities using a sigmoid function.\n",
    "Assumes a linear relationship between features and the log-odds of the target.\n",
    "Simple, fast, and interpretable  great baseline for linearly separable data.\n",
    "\n",
    "-  Support Vector Machine (SVM)\n",
    "\n",
    "Finds the optimal hyperplane that maximizes the margin between classes.\n",
    "Works well in high-dimensional spaces.\n",
    "Can use different kernels (linear, RBF, polynomial) to capture nonlinear patterns.\n",
    "Sensitive to scaling; may be slower on large datasets.\n",
    "\n",
    "-  K-Nearest Neighbors (KNN)\n",
    "\n",
    "Instance-based learning  no training, just storing.\n",
    "Classifies based on the majority label among k-nearest neighbors.\n",
    "Simple and intuitive, but slow with large datasets.\n",
    "Sensitive to feature scaling and irrelevant features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python-312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
