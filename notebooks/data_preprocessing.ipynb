{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import graphviz\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "%matplotlib inline\n",
    "\n",
    "# Stats\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats import shapiro, norm\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# typing\n",
    "from typing import Dict,List\n",
    "\n",
    "# os\n",
    "import os\n",
    "\n",
    "# time\n",
    "import time\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(\"../config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except:\n",
    "    print(\"Yaml configuration file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = pd.read_csv(config[\"data\"][\"raw\"][\"file_scraped\"])\n",
    "df_verified = pd.read_csv(config[\"data\"][\"raw\"][\"file_verified\"])\n",
    "df_vulnerable = pd.read_csv(config[\"data\"][\"raw\"][\"file_vulnerable\"])\n",
    "# df_scraped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = {\"scraped\": df_scraped, \"verified\": df_verified, \"vulnerable\": df_vulnerable}\n",
    "for key, val in raw_files.items():\n",
    "        print (f\"Dimension of file '{key}': {val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped[\"last_updated\"] = pd.to_datetime(df_scraped[\"last_updated\"], errors =\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean column names and remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = {\"scraped\": df_scraped, \"verified\": df_verified, \"vulnerable\": df_vulnerable}\n",
    "for val in raw_files.values():\n",
    "    val.columns = val.columns.str.strip().str.lower().str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = df_scraped.drop([\"id\", \"description\"], axis = 1)\n",
    "df_verified = df_verified.drop([\"publisher\", \"install_count\"],axis = 1).rename(columns = {\"extension_name\": \"name\",\"source_code\":\"repository\"})\n",
    "df_vulnerable = df_vulnerable.drop([\"repository_name\", \"critical_vulnerability_names\", \"high_vulnerability_names\", \"medium_vulnerability_names\", \"low_vulnerability_names\"],axis = 1).rename(columns = {\"extension_name\": \"name\", \"repository_link\":\"repository\"})\n",
    "\n",
    "df_scraped.columns, df_verified.columns, df_vulnerable.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check duplicated and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = df_scraped.drop_duplicates(subset=[\"name\",\"publisher\"])\n",
    "df_verified = df_verified.drop_duplicates(subset=[\"name\",\"repository\"])\n",
    "df_vulnerable = df_vulnerable.drop_duplicates(subset=[\"name\",\"repository\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check null values and drop null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verified = df_verified[~df_verified[\"repository\"].isna()]\n",
    "df_vulnerable = df_vulnerable[~df_vulnerable[\"name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = {\"scraped\": df_scraped, \"verified\": df_verified, \"vulnerable\": df_vulnerable}\n",
    "for key, val in raw_files.items():\n",
    "        print (f\"Dimension of '{key}' after dropping null and duplicated: {val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine raw df after cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_clean = pd.merge(df_verified,df_vulnerable, on=[\"name\", \"repository\"])\n",
    "df_clean = pd.merge(df_scraped, df_pre_clean, on = \"name\")\n",
    "df_clean[\"repository\"] = [x[:-4] if x.endswith(\".git\") else x for x in df_clean[\"repository\"]]\n",
    "df_clean.to_csv(config[\"data\"][\"clean\"][\"file_cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config[\"data\"][\"clean\"][\"file_cleaned\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
